{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95858434-1597-4bf0-961a-58d3283c8af2",
   "metadata": {},
   "source": [
    "# Hugging Face Demo \n",
    "Code from overview document.\n",
    "\n",
    "## Set up a special HF environment \n",
    "1. install required libraries (M1 has special \n",
    "2. install ipykernel\n",
    "3. add to jupyter `python -m ipykernel install --user --name=hf_env --display-name \"Python (hf_env)\"`\n",
    "4. restart jupyter\n",
    "\n",
    "## Packages (in order)\n",
    "1. torch\n",
    "1. torchvision \n",
    "1. torchaudio\n",
    "1. transformers\n",
    "1. tf-keras\n",
    "1. pandas\n",
    "1. numpy\n",
    "1. matplotlib\n",
    "\n",
    "## M1/M2/M3 Macs ... \n",
    "- You have to use the CPU (not GPU) no CUDA support.\n",
    "- torch, torchvision, torchaudio are different\n",
    "- `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu`\n",
    "\n",
    "---\n",
    "## Simple Test Script\n",
    "**Output**: `{'sequence': 'Climate change is a significant challenge for the planet.', 'labels': ['environment', 'policy', 'economics'], 'scores': [0.9726617336273193, 0.012952088378369808, 0.00014581126742996275]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663257d5-44b7-4a0b-82d3-9bde7c829faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Climate change is a significant challenge for the planet.', 'labels': ['environment', 'policy', 'economics'], 'scores': [0.9726617336273193, 0.012952088378369808, 0.00014581126742996275]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Test text\n",
    "text = \"Climate change is a significant challenge for the planet.\"\n",
    "labels = [\"environment\", \"policy\", \"economics\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(text, candidate_labels=labels, multi_label=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66779a0-c502-438e-9a25-632a6dc050f0",
   "metadata": {},
   "source": [
    "## Load a pre-trained model for zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65509992-30d7-495f-9dd5-ea31fb41422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts extracted:\n",
      "environment: 0.71\n",
      "policy: 0.64\n",
      "science: 0.11\n",
      "technology: 0.00\n",
      "health: 0.00\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained model for zero-shot classification\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "Climate change is impacting agriculture and biodiversity worldwide. \n",
    "Governments are implementing policies to reduce carbon emissions and promote renewable energy.\n",
    "\"\"\"\n",
    "\n",
    "# Define candidate labels\n",
    "labels = [\"environment\", \"policy\", \"science\", \"technology\", \"health\"]\n",
    "\n",
    "# Perform classification\n",
    "results = classifier(text, candidate_labels=labels, multi_label=True)\n",
    "\n",
    "# Print results\n",
    "print(\"Concepts extracted:\")\n",
    "for label, score in zip(results[\"labels\"], results[\"scores\"]):\n",
    "    print(f\"{label}: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef20f5-1bfb-4a48-aded-680f7fed31c1",
   "metadata": {},
   "source": [
    "## Census Example using Public Metadata\n",
    "**NOTE**: This creates sample data as provided in the overview paper for an 'all in one' example here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23850369-f9db-47a8-a20d-d753c7a20774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b603dc34-2cc7-4bb1-8557-baf6735d3988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed metadata saved to processed_census_metadata.csv\n",
      "                  SurveyID                                           Metadata  \\\n",
      "0      POPESTprmagesex2014  Annual Estimates of the Resident Population by...   \n",
      "1  POPESTPROJagegroups2014  Projected Population by Age Groups, Sex, Race,...   \n",
      "2     POPESTPROJbirths2014  Projected Births by Sex, Race, and Hispanic Or...   \n",
      "3     POPESTPROJdeaths2014  Projected Deaths by Single Year of Age, Sex, R...   \n",
      "4        POPESTPROJnat2014  Projected Population by Single Year of Age, Se...   \n",
      "\n",
      "                                  Extracted Concepts  \n",
      "0  {'population estimates': 0.9928871989250183, '...  \n",
      "1  {'projections': 0.9980393648147583, 'populatio...  \n",
      "2  {'birth data': 0.6991598606109619, 'projection...  \n",
      "3  {'projections': 0.996796190738678, 'death data...  \n",
      "4  {'projections': 0.9980165958404541, 'populatio...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Sample data based on the provided example\n",
    "data = {\n",
    "    \"SurveyID\": [\n",
    "        \"POPESTprmagesex2014\",\n",
    "        \"POPESTPROJagegroups2014\",\n",
    "        \"POPESTPROJbirths2014\",\n",
    "        \"POPESTPROJdeaths2014\",\n",
    "        \"POPESTPROJnat2014\",\n",
    "    ],\n",
    "    \"Metadata\": [\n",
    "        \"Annual Estimates of the Resident Population by Five-Year Age Groups and Sex for the Municipios of Puerto Rico. Source: U.S. Census Bureau, Population Division. Note: The estimates are based on the 2010 Census and reflect changes to the April 1, 2010 population due to the Count Question Resolution program and geographic program revisions.\",\n",
    "        \"Projected Population by Age Groups, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: 'In combination' means in combination with one or more other races. The sum of the five race-in-combination groups adds to more than the total population because individuals may report more than one race.\",\n",
    "        \"Projected Births by Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race. All projected births are considered native born.\",\n",
    "        \"Projected Deaths by Single Year of Age, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race.\",\n",
    "        \"Projected Population by Single Year of Age, Sex, Race, Hispanic Origin, and Nativity for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Convert the sample data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define candidate labels\n",
    "categories = [\n",
    "    \"population estimates\",\n",
    "    \"migration\",\n",
    "    \"housing data\",\n",
    "    \"economic data\",\n",
    "    \"birth data\",\n",
    "    \"death data\",\n",
    "    \"projections\",\n",
    "    \"geographic information\",\n",
    "    \"race and ethnicity\",\n",
    "]\n",
    "\n",
    "# Function to classify metadata\n",
    "def classify_metadata(text):\n",
    "    result = classifier(text, candidate_labels=categories, multi_label=True)\n",
    "    return {label: score for label, score in zip(result[\"labels\"], result[\"scores\"])}\n",
    "\n",
    "# Apply classification to the Metadata column\n",
    "df[\"Extracted Concepts\"] = df[\"Metadata\"].apply(classify_metadata)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"processed_census_metadata.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed metadata saved to {output_path}\")\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325cbc9a-6c73-4b6f-bb5e-9bce6e1d908e",
   "metadata": {},
   "source": [
    "# Super Bonus! Pipeline Implementation with Subcategories\n",
    "### **Pipeline Implementation with Subcategories**\n",
    "\n",
    "Our **Pipeline Implementation with Subcategories** enriches Census metadata by leveraging **Hugging Face models** to extract both high-level categories and granular subcategories (concepts) from metadata descriptions. The steps are as follows:\n",
    "\n",
    "1. **Classify High-Level Categories**:\n",
    "   - Using **Zero-Shot Classification** (e.g., `facebook/bart-large-mnli`), the pipeline assigns each metadata description a broad category such as `Demographics`, `Economics`, `Housing`, etc.\n",
    "   - This provides an overarching organizational theme for each variable.\n",
    "\n",
    "2. **Extract Subcategories (Concepts)**:\n",
    "   - Using **Named Entity Recognition (NER)** (e.g., `dslim/bert-base-NER`), the pipeline identifies detailed terms or concepts embedded in the metadata descriptions.\n",
    "   - These concepts include granular details such as \"Sex,\" \"Industry,\" or \"Median Earnings,\" which serve as key nodes in the knowledge graph.\n",
    "\n",
    "3. **Enrich Metadata and Create Relationships**:\n",
    "   - The enriched dataset includes both the assigned category and extracted concepts for each metadata description.\n",
    "   - Relationships are built for the knowledge graph:\n",
    "     - `BELONGS_TO`: Links metadata descriptions to their high-level category.\n",
    "     - `HAS_CONCEPT`: Links metadata descriptions to specific concepts extracted via NER.\n",
    "\n",
    "4. **Prepare Data for Knowledge Graph Integration**:\n",
    "   - Outputs an enriched DataFrame and a relationships DataFrame for direct import into graph databases like Neo4j.\n",
    "\n",
    "This approach provides a structured and scalable way to organize metadata, enabling meaningful relationships and efficient querying in the knowledge graph.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Named Entity Recognition (NER) Is the Best Choice**\n",
    "\n",
    "#### **What Is NER?**\n",
    "Named Entity Recognition (NER) is a technique used in Natural Language Processing (NLP) to identify and extract key terms or entities from text. Entities can include people, organizations, locations, or, in our case, **concepts and subcategories** from metadata descriptions.\n",
    "\n",
    "#### **Why Is NER Effective?**\n",
    "- **Pre-Trained Knowledge**: NER models, such as `dslim/bert-base-NER`, are trained on large corpora to identify entities with high accuracy, even without fine-tuning.\n",
    "- **Out-of-the-Box Performance**: NER works immediately without requiring extensive setup or labeled data.\n",
    "- **Granularity**: Extracts detailed, contextually relevant terms (e.g., \"Sex,\" \"Industry\") that serve as concepts for the knowledge graph.\n",
    "- **Scalability**: Handles large datasets efficiently, adapting to various text lengths and structures.\n",
    "\n",
    "#### **Other Possible Choices**\n",
    "1. **Rule-Based Extraction**:\n",
    "   - Relies on predefined patterns or regular expressions to extract key terms.\n",
    "   - **Strengths**: High precision for structured or repetitive text.\n",
    "   - **Weaknesses**: Fragile, hard to scale, and misses subtle variations in text.\n",
    "\n",
    "2. **Clustering or Topic Modeling**:\n",
    "   - Groups metadata descriptions into clusters or topics based on semantic similarity.\n",
    "   - **Strengths**: Useful for finding hidden patterns or relationships.\n",
    "   - **Weaknesses**: Difficult to interpret; doesnâ€™t extract specific entities.\n",
    "\n",
    "3. **Fine-Tuning a Transformer Model**:\n",
    "   - Adapts a model like `bert-base-uncased` to your dataset by training it on labeled metadata.\n",
    "   - **Strengths**: Custom-fit for your domain.\n",
    "   - **Weaknesses**: Requires significant computational resources and labeled data.\n",
    "\n",
    "#### **Decision Criteria and Analysis**\n",
    "| **Criteria**         | **NER**                     | **Rule-Based**             | **Clustering/Topics**      | **Fine-Tuning**            |\n",
    "|-----------------------|-----------------------------|----------------------------|----------------------------|----------------------------|\n",
    "| Setup Effort          | Low                        | Medium                     | Medium                     | High                       |\n",
    "| Scalability           | High                       | Low                        | Medium                     | Medium                     |\n",
    "| Granularity           | High                       | Medium                     | Low                        | High                       |\n",
    "| Flexibility           | High                       | Low                        | High                       | High                       |\n",
    "| Computational Cost    | Low                        | Low                        | Medium                     | High                       |\n",
    "\n",
    "NER strikes the best balance of **ease of use**, **accuracy**, and **scalability**. While fine-tuning offers more customization and clustering finds broader themes, NERâ€™s ability to extract precise and meaningful concepts makes it the most effective and efficient choice for enriching Census metadata.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7de7de-062b-482f-acef-786771d91adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched Metadata DataFrame:\n",
      "                                            metadata      Category  \\\n",
      "0  Annual Estimates of the Resident Population by...  Demographics   \n",
      "1  Projected Population by Age Groups, Sex, Race,...  Demographics   \n",
      "2  Projected Births by Sex, Race, and Hispanic Or...  Demographics   \n",
      "3  Projected Deaths by Single Year of Age, Sex, R...  Demographics   \n",
      "4  Projected Population by Single Year of Age, Se...  Demographics   \n",
      "\n",
      "                                            Concepts  \n",
      "0  [Puerto, Rico, U, ., S, ., Census, Bureau, Div...  \n",
      "1  [Hispanic, United, States, U, ., S, ., Census,...  \n",
      "2  [Hispanic, United, States, U, ., S, ., Census,...  \n",
      "3  [Hispanic, United, States, U, ., S, ., Census,...  \n",
      "4  [Hispanic, United, States, U, ., S, ., Census,...  \n",
      "\n",
      "Relationships DataFrame:\n",
      "                                               source        target  \\\n",
      "0   Annual Estimates of the Resident Population by...  Demographics   \n",
      "1   Annual Estimates of the Resident Population by...        Puerto   \n",
      "2   Annual Estimates of the Resident Population by...          Rico   \n",
      "3   Annual Estimates of the Resident Population by...             U   \n",
      "4   Annual Estimates of the Resident Population by...             .   \n",
      "..                                                ...           ...   \n",
      "59  Projected Population by Single Year of Age, Se...        Census   \n",
      "60  Projected Population by Single Year of Age, Se...        Bureau   \n",
      "61  Projected Population by Single Year of Age, Se...      Division   \n",
      "62  Projected Population by Single Year of Age, Se...      Hispanic   \n",
      "63  Projected Population by Single Year of Age, Se...      Hispanic   \n",
      "\n",
      "   relationship  \n",
      "0    BELONGS_TO  \n",
      "1   HAS_CONCEPT  \n",
      "2   HAS_CONCEPT  \n",
      "3   HAS_CONCEPT  \n",
      "4   HAS_CONCEPT  \n",
      "..          ...  \n",
      "59  HAS_CONCEPT  \n",
      "60  HAS_CONCEPT  \n",
      "61  HAS_CONCEPT  \n",
      "62  HAS_CONCEPT  \n",
      "63  HAS_CONCEPT  \n",
      "\n",
      "[64 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load Hugging Face pipelines\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "# Predefined categories (high-level)\n",
    "categories = [\n",
    "    \"Demographics\", \"Economics\", \"Housing\", \"Education\", \"Employment\",\n",
    "    \"Health\", \"Geography\", \"Population Density\", \"Social Characteristics\", \"Environment\"\n",
    "]\n",
    "\n",
    "# Assume you have a DataFrame with a column called 'metadata'\n",
    "metadata_df = pd.DataFrame({\n",
    "    \"metadata\": [\n",
    "        \"Annual Estimates of the Resident Population by Five-Year Age Groups and Sex for the Municipios of Puerto Rico. Source: U.S. Census Bureau, Population Division. Note: The estimates are based on the 2010 Census and reflect changes to the April 1, 2010 population due to the Count Question Resolution program and geographic program revisions.\",\n",
    "        \"Projected Population by Age Groups, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: 'In combination' means in combination with one or more other races. The sum of the five race-in-combination groups adds to more than the total population because individuals may report more than one race.\",\n",
    "        \"Projected Births by Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race. All projected births are considered native born.\",\n",
    "        \"Projected Deaths by Single Year of Age, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race.\",\n",
    "        \"Projected Population by Single Year of Age, Sex, Race, Hispanic Origin, and Nativity for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race.\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Step 1: Classify High-Level Categories\n",
    "def classify_category(description):\n",
    "    result = zero_shot_classifier(description, categories)\n",
    "    return result[\"labels\"][0]  # Top category\n",
    "\n",
    "metadata_df[\"Category\"] = metadata_df[\"metadata\"].apply(classify_category)\n",
    "\n",
    "# Step 2: Extract Subcategories or Concepts\n",
    "def extract_concepts(description):\n",
    "    entities = ner_pipeline(description)\n",
    "    return [entity[\"word\"] for entity in entities]\n",
    "\n",
    "metadata_df[\"Concepts\"] = metadata_df[\"metadata\"].apply(extract_concepts)\n",
    "\n",
    "# Step 3: Enrich with Relationships (High-Level + Subcategories)\n",
    "relationships = []\n",
    "for _, row in metadata_df.iterrows():\n",
    "    metadata_description = row[\"metadata\"]\n",
    "    category_node = row[\"Category\"]\n",
    "    concept_nodes = row[\"Concepts\"]\n",
    "    \n",
    "    # Add relationship to high-level category\n",
    "    relationships.append({\n",
    "        \"source\": metadata_description,\n",
    "        \"target\": category_node,\n",
    "        \"relationship\": \"BELONGS_TO\"\n",
    "    })\n",
    "    \n",
    "    # Add relationships to concepts (subcategories)\n",
    "    for concept in concept_nodes:\n",
    "        relationships.append({\n",
    "            \"source\": metadata_description,\n",
    "            \"target\": concept,\n",
    "            \"relationship\": \"HAS_CONCEPT\"\n",
    "        })\n",
    "\n",
    "# Convert relationships to DataFrame for export\n",
    "relationships_df = pd.DataFrame(relationships)\n",
    "\n",
    "# Output enriched metadata and relationships\n",
    "print(\"Enriched Metadata DataFrame:\")\n",
    "print(metadata_df)\n",
    "\n",
    "print(\"\\nRelationships DataFrame:\")\n",
    "print(relationships_df)\n",
    "\n",
    "# Save enriched metadata and relationships\n",
    "metadata_df.to_csv(\"enriched_metadata_with_subcategories.csv\", index=False)\n",
    "relationships_df.to_csv(\"relationships_with_subcategories.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d5f87-8cb6-4ce1-8f59-716d9de0edd5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b15e397c-a15a-4f5c-8b71-8bc10116c0a1",
   "metadata": {},
   "source": [
    "# **Issue with NER**\n",
    "Named Entity Recognition (NER) struggled to effectively extract meaningful concepts from Census metadata due to:\n",
    "- **Misinterpretation**: Fragmentation of terms (e.g., splitting \"Hispanic Origin\" into \"Hispanic\" and \"Origin\").\n",
    "- **Lack of Domain Knowledge**: General-purpose NER models are not trained on Census-specific terminology, leading to poor recognition of key phrases.\n",
    "- **Noisy Outputs**: Punctuation and irrelevant terms (e.g., \"U\", \".\", \"S\") were extracted as entities, resulting in incorrect relationships in the knowledge graph.\n",
    "\n",
    "---\n",
    "\n",
    "## **What Would Be Needed to Fix NER**\n",
    "To make NER work, we would need to:\n",
    "1. **Preprocess Descriptions**: Remove punctuation, stopwords, and irrelevant terms before running NER.\n",
    "2. **Postprocess Results**: Filter out noisy outputs and refine extracted entities to better match domain-specific concepts.\n",
    "3. **Add Custom Rules**: Implement regex or predefined patterns to capture phrases missed by NER.\n",
    "4. **Fine-Tune NER Models**: Train an NER model on labeled Census metadata to improve accuracy for domain-specific terms.\n",
    "\n",
    "While these steps could work, they would require significant development, testing, and ongoing refinement for scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## **Using ChatGPT for Concept Extraction**\n",
    "ChatGPT addresses these challenges by:\n",
    "- **Understanding Context**: ChatGPTâ€™s pre-trained knowledge includes Census-related terminology, avoiding fragmentation or misinterpretation of key phrases.\n",
    "- **Dynamic Flexibility**: Allows for extracting both high-level categories (e.g., \"Demographics\") and granular concepts (e.g., \"Hispanic Origin\") in a single prompt.\n",
    "- **Simplified Pipeline**: Eliminates the need for extensive preprocessing, postprocessing, or fine-tuning, providing accurate outputs directly.\n",
    "- **Scalable and Adaptable**: Easily processes large datasets and adapts to new descriptions with minimal adjustments to prompts.\n",
    "\n",
    "This approach reduces complexity and ensures consistent, domain-specific concept extraction with high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ed5c991-ddfb-4917-9ce8-2b21727c6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched Metadata:\n",
      "                                            metadata  \\\n",
      "0  Annual Estimates of the Resident Population by...   \n",
      "1  Projected Population by Age Groups, Sex, Race,...   \n",
      "2  Projected Births by Sex, Race, and Hispanic Or...   \n",
      "3  Projected Deaths by Single Year of Age, Sex, R...   \n",
      "4  Projected Population by Single Year of Age, Se...   \n",
      "\n",
      "                                          Enrichment        Category  \\\n",
      "0  High-Level Category: [Demographics]\\nConcepts:...  [Demographics]   \n",
      "1  High-Level Category: [Demographics]\\nConcepts:...  [Demographics]   \n",
      "2  High-Level Category: [Demographics]\\nConcepts:...  [Demographics]   \n",
      "3  High-Level Category: [Demographics]\\nConcepts:...  [Demographics]   \n",
      "4  High-Level Category: [Demographics]\\nConcepts:...  [Demographics]   \n",
      "\n",
      "                                            Concepts  \n",
      "0  [Annual Estimates, Resident Population, Five-Y...  \n",
      "1  [Projected Population, Age Groups, Sex, Race, ...  \n",
      "2  [Projected Births, Sex, Race, Hispanic Origin,...  \n",
      "3  [Projected Deaths, Single Year of Age, Sex, Ra...  \n",
      "4  [Projected Population, Single Year of Age, Sex...  \n",
      "\n",
      "Relationships for Knowledge Graph:\n",
      "        source                             target relationship source_type  \\\n",
      "0   Metadata_1                     [Demographics]   BELONGS_TO    Metadata   \n",
      "1   Metadata_1                   Annual Estimates  HAS_CONCEPT    Metadata   \n",
      "2   Metadata_1                Resident Population  HAS_CONCEPT    Metadata   \n",
      "3   Metadata_1               Five-Year Age Groups  HAS_CONCEPT    Metadata   \n",
      "4   Metadata_1                                Sex  HAS_CONCEPT    Metadata   \n",
      "5   Metadata_1                         Municipios  HAS_CONCEPT    Metadata   \n",
      "6   Metadata_1                        Puerto Rico  HAS_CONCEPT    Metadata   \n",
      "7   Metadata_1                 U.S. Census Bureau  HAS_CONCEPT    Metadata   \n",
      "8   Metadata_1                Population Division  HAS_CONCEPT    Metadata   \n",
      "9   Metadata_1                        2010 Census  HAS_CONCEPT    Metadata   \n",
      "10  Metadata_1  Count Question Resolution program  HAS_CONCEPT    Metadata   \n",
      "11  Metadata_1       geographic program revisions  HAS_CONCEPT    Metadata   \n",
      "12  Metadata_2                     [Demographics]   BELONGS_TO    Metadata   \n",
      "13  Metadata_2               Projected Population  HAS_CONCEPT    Metadata   \n",
      "14  Metadata_2                         Age Groups  HAS_CONCEPT    Metadata   \n",
      "15  Metadata_2                                Sex  HAS_CONCEPT    Metadata   \n",
      "16  Metadata_2                               Race  HAS_CONCEPT    Metadata   \n",
      "17  Metadata_2                    Hispanic Origin  HAS_CONCEPT    Metadata   \n",
      "18  Metadata_2                      United States  HAS_CONCEPT    Metadata   \n",
      "19  Metadata_2                          2014-2060  HAS_CONCEPT    Metadata   \n",
      "20  Metadata_2                 U.S. Census Bureau  HAS_CONCEPT    Metadata   \n",
      "21  Metadata_2                Population Division  HAS_CONCEPT    Metadata   \n",
      "22  Metadata_2         Race-in-combination groups  HAS_CONCEPT    Metadata   \n",
      "23  Metadata_3                     [Demographics]   BELONGS_TO    Metadata   \n",
      "24  Metadata_3                   Projected Births  HAS_CONCEPT    Metadata   \n",
      "25  Metadata_3                                Sex  HAS_CONCEPT    Metadata   \n",
      "26  Metadata_3                               Race  HAS_CONCEPT    Metadata   \n",
      "27  Metadata_3                    Hispanic Origin  HAS_CONCEPT    Metadata   \n",
      "28  Metadata_3                      United States  HAS_CONCEPT    Metadata   \n",
      "29  Metadata_3                 U.S. Census Bureau  HAS_CONCEPT    Metadata   \n",
      "30  Metadata_3                Population Division  HAS_CONCEPT    Metadata   \n",
      "31  Metadata_3                          Ethnicity  HAS_CONCEPT    Metadata   \n",
      "32  Metadata_3                        Native Born  HAS_CONCEPT    Metadata   \n",
      "33  Metadata_4                     [Demographics]   BELONGS_TO    Metadata   \n",
      "34  Metadata_4                   Projected Deaths  HAS_CONCEPT    Metadata   \n",
      "35  Metadata_4                 Single Year of Age  HAS_CONCEPT    Metadata   \n",
      "36  Metadata_4                                Sex  HAS_CONCEPT    Metadata   \n",
      "37  Metadata_4                               Race  HAS_CONCEPT    Metadata   \n",
      "38  Metadata_4                    Hispanic Origin  HAS_CONCEPT    Metadata   \n",
      "39  Metadata_4           United States: 2014-2060  HAS_CONCEPT    Metadata   \n",
      "40  Metadata_4                 U.S. Census Bureau  HAS_CONCEPT    Metadata   \n",
      "41  Metadata_4                Population Division  HAS_CONCEPT    Metadata   \n",
      "42  Metadata_4                          Ethnicity  HAS_CONCEPT    Metadata   \n",
      "43  Metadata_5                     [Demographics]   BELONGS_TO    Metadata   \n",
      "44  Metadata_5               Projected Population  HAS_CONCEPT    Metadata   \n",
      "45  Metadata_5                 Single Year of Age  HAS_CONCEPT    Metadata   \n",
      "46  Metadata_5                                Sex  HAS_CONCEPT    Metadata   \n",
      "47  Metadata_5                               Race  HAS_CONCEPT    Metadata   \n",
      "48  Metadata_5                    Hispanic Origin  HAS_CONCEPT    Metadata   \n",
      "49  Metadata_5                           Nativity  HAS_CONCEPT    Metadata   \n",
      "50  Metadata_5                      United States  HAS_CONCEPT    Metadata   \n",
      "51  Metadata_5                          2014-2060  HAS_CONCEPT    Metadata   \n",
      "52  Metadata_5                 U.S. Census Bureau  HAS_CONCEPT    Metadata   \n",
      "53  Metadata_5                Population Division  HAS_CONCEPT    Metadata   \n",
      "54  Metadata_5                          Ethnicity  HAS_CONCEPT    Metadata   \n",
      "\n",
      "   target_type  \n",
      "0     Category  \n",
      "1      Concept  \n",
      "2      Concept  \n",
      "3      Concept  \n",
      "4      Concept  \n",
      "5      Concept  \n",
      "6      Concept  \n",
      "7      Concept  \n",
      "8      Concept  \n",
      "9      Concept  \n",
      "10     Concept  \n",
      "11     Concept  \n",
      "12    Category  \n",
      "13     Concept  \n",
      "14     Concept  \n",
      "15     Concept  \n",
      "16     Concept  \n",
      "17     Concept  \n",
      "18     Concept  \n",
      "19     Concept  \n",
      "20     Concept  \n",
      "21     Concept  \n",
      "22     Concept  \n",
      "23    Category  \n",
      "24     Concept  \n",
      "25     Concept  \n",
      "26     Concept  \n",
      "27     Concept  \n",
      "28     Concept  \n",
      "29     Concept  \n",
      "30     Concept  \n",
      "31     Concept  \n",
      "32     Concept  \n",
      "33    Category  \n",
      "34     Concept  \n",
      "35     Concept  \n",
      "36     Concept  \n",
      "37     Concept  \n",
      "38     Concept  \n",
      "39     Concept  \n",
      "40     Concept  \n",
      "41     Concept  \n",
      "42     Concept  \n",
      "43    Category  \n",
      "44     Concept  \n",
      "45     Concept  \n",
      "46     Concept  \n",
      "47     Concept  \n",
      "48     Concept  \n",
      "49     Concept  \n",
      "50     Concept  \n",
      "51     Concept  \n",
      "52     Concept  \n",
      "53     Concept  \n",
      "54     Concept  \n",
      "\n",
      "Metadata Mapping:\n",
      "  metadata_id                                      full_metadata\n",
      "0  Metadata_1  Annual Estimates of the Resident Population by...\n",
      "1  Metadata_2  Projected Population by Age Groups, Sex, Race,...\n",
      "2  Metadata_3  Projected Births by Sex, Race, and Hispanic Or...\n",
      "3  Metadata_4  Projected Deaths by Single Year of Age, Sex, R...\n",
      "4  Metadata_5  Projected Population by Single Year of Age, Se...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your metadata descriptions into a DataFrame\n",
    "metadata_df = pd.DataFrame({\n",
    "    \"metadata\": [\n",
    "        \"Annual Estimates of the Resident Population by Five-Year Age Groups and Sex for the Municipios of Puerto Rico. Source: U.S. Census Bureau, Population Division. Note: The estimates are based on the 2010 Census and reflect changes to the April 1, 2010 population due to the Count Question Resolution program and geographic program revisions.\",\n",
    "        \"Projected Population by Age Groups, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: 'In combination' means in combination with one or more other races. The sum of the five race-in-combination groups adds to more than the total population because individuals may report more than one race.\",\n",
    "        \"Projected Births by Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race. All projected births are considered native born.\",\n",
    "        \"Projected Deaths by Single Year of Age, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race.\",\n",
    "        \"Projected Population by Single Year of Age, Sex, Race, Hispanic Origin, and Nativity for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race.\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "import os\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your metadata descriptions into a DataFrame\n",
    "metadata_df = pd.DataFrame({\n",
    "    \"metadata\": [\n",
    "        \"Annual Estimates of the Resident Population by Five-Year Age Groups and Sex for the Municipios of Puerto Rico. Source: U.S. Census Bureau, Population Division. Note: The estimates are based on the 2010 Census and reflect changes to the April 1, 2010 population due to the Count Question Resolution program and geographic program revisions.\",\n",
    "        \"Projected Population by Age Groups, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: 'In combination' means in combination with one or more other races. The sum of the five race-in-combination groups adds to more than the total population because individuals may report more than one race.\",\n",
    "        \"Projected Births by Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race. All projected births are considered native born.\",\n",
    "        \"Projected Deaths by Single Year of Age, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race.\",\n",
    "        \"Projected Population by Single Year of Age, Sex, Race, Hispanic Origin, and Nativity for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division. Note: Hispanic origin is considered an ethnicity, not a race. Hispanics may be of any race.\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "def extract_metadata_enrichment(description):\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the following from the metadata description in a structured format:\n",
    "        1. High-Level Category: The overarching category this description belongs to (e.g., Demographics, Economics, Housing).\n",
    "        2. Concepts: List key terms or phrases that describe the metadata in brackets, separated by commas (e.g., [Population, Hispanic Origin, Age Groups])\n",
    "        \n",
    "        Format your response exactly like this:\n",
    "        High-Level Category: [category]\n",
    "        Concepts: [concept1, concept2, concept3]\n",
    "        \n",
    "        Metadata: \"{description}\"\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant that extracts metadata enrichment in a structured format.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing metadata: {description}\\nError: {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply the API call to each row in the DataFrame\n",
    "metadata_df[\"Enrichment\"] = metadata_df[\"metadata\"].apply(extract_metadata_enrichment)\n",
    "\n",
    "def parse_enrichment(enrichment):\n",
    "    if enrichment is None:\n",
    "        return pd.Series([None, None])\n",
    "    try:\n",
    "        # Extract Category and Concepts using regex\n",
    "        category_match = re.search(r'High-Level Category:\\s*(.+?)(?=\\n|$)', enrichment)\n",
    "        concepts_match = re.search(r'Concepts:\\s*\\[(.*?)\\]', enrichment)\n",
    "        \n",
    "        if category_match and concepts_match:\n",
    "            category = category_match.group(1).strip()\n",
    "            # Split concepts and clean up each concept\n",
    "            concepts = [concept.strip() for concept in concepts_match.group(1).split(',')]\n",
    "            return pd.Series([category, concepts])\n",
    "        return pd.Series([None, None])\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing enrichment: {e}\")\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "# Parse and add extracted data\n",
    "metadata_df[[\"Category\", \"Concepts\"]] = metadata_df[\"Enrichment\"].apply(parse_enrichment)\n",
    "\n",
    "# Generate relationships for the knowledge graph\n",
    "relationships = []\n",
    "\n",
    "for idx, row in metadata_df.iterrows():\n",
    "    metadata_description = row[\"metadata\"]\n",
    "    category_node = row[\"Category\"]\n",
    "    concept_nodes = row[\"Concepts\"]\n",
    "    \n",
    "    # Create a shorter identifier for the metadata\n",
    "    metadata_id = f\"Metadata_{idx + 1}\"\n",
    "    \n",
    "    # Add relationship to the category\n",
    "    if category_node and isinstance(category_node, str):\n",
    "        relationships.append({\n",
    "            \"source\": metadata_id,\n",
    "            \"target\": category_node,\n",
    "            \"relationship\": \"BELONGS_TO\",\n",
    "            \"source_type\": \"Metadata\",\n",
    "            \"target_type\": \"Category\"\n",
    "        })\n",
    "    \n",
    "    # Add relationships to concepts\n",
    "    if concept_nodes and isinstance(concept_nodes, list):\n",
    "        for concept in concept_nodes:\n",
    "            if concept:  # Check if concept is not empty\n",
    "                relationships.append({\n",
    "                    \"source\": metadata_id,\n",
    "                    \"target\": concept.strip(),\n",
    "                    \"relationship\": \"HAS_CONCEPT\",\n",
    "                    \"source_type\": \"Metadata\",\n",
    "                    \"target_type\": \"Concept\"\n",
    "                })\n",
    "\n",
    "# Convert relationships to a DataFrame\n",
    "relationships_df = pd.DataFrame(relationships)\n",
    "\n",
    "# Create a mapping DataFrame to store metadata IDs and their full descriptions\n",
    "metadata_mapping = pd.DataFrame({\n",
    "    'metadata_id': [f\"Metadata_{i+1}\" for i in range(len(metadata_df))],\n",
    "    'full_metadata': metadata_df['metadata']\n",
    "})\n",
    "\n",
    "# Save results\n",
    "metadata_df.to_csv(\"enriched_metadata_chatgpt.csv\", index=False)\n",
    "relationships_df.to_csv(\"relationships_chatgpt.csv\", index=False)\n",
    "metadata_mapping.to_csv(\"metadata_mapping_chatgpt.csv\", index=False)\n",
    "\n",
    "print(\"Enriched Metadata:\")\n",
    "print(metadata_df)\n",
    "\n",
    "print(\"\\nRelationships for Knowledge Graph:\")\n",
    "print(relationships_df)\n",
    "\n",
    "print(\"\\nMetadata Mapping:\")\n",
    "print(metadata_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718fa70-96fa-4521-9c52-9fd73186754a",
   "metadata": {},
   "source": [
    "# But we have an Issue: Generic Redundant Information: \n",
    "\n",
    "## **Issue Summary**\n",
    "When processing metadata for the knowledge graph, we encountered the following challenges:\n",
    "1. **Redundant Information**:\n",
    "   - Generic terms like \"US Census Bureau\" added noise without value.\n",
    "   - Variations of terms (e.g., \"Population Division\" vs. \"Division of Population Studies\") caused inconsistencies.\n",
    "2. **Scaling**:\n",
    "   - Cleaning and normalizing millions of rows manually was infeasible.\n",
    "3. **Future Data Sources**:\n",
    "   - Other agencies might introduce new terms, requiring scalable and adaptable handling.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solution**\n",
    "To address these challenges, we implemented a **dynamic normalization pipeline** using:\n",
    "1. **Predefined Normalization Dictionary**:\n",
    "   - Common terms are mapped to their canonical forms (e.g., `\"US Census Bureau\" â†’ \"Census Bureau\"`).\n",
    "   - Ensures consistent terminology from the start.\n",
    "\n",
    "2. **Dynamic Matching with RapidFuzz**:\n",
    "   - Automatically detects variations of terms using similarity scoring (`score_cutoff=80`).\n",
    "   - Matches terms not explicitly listed in the dictionary but similar enough to known values.\n",
    "\n",
    "3. **Hierarchical Relationships**:\n",
    "   - Structured the knowledge graph to include clear parent-child relationships:\n",
    "     - Example: `\"Population Division BELONGS_TO Census Bureau\"`\n",
    "\n",
    "4. **Iterative Refinement**:\n",
    "   - Logs new terms not in the dictionary to `new_terms_log.txt` for review and potential inclusion.\n",
    "   - Allows the normalization dictionary to grow dynamically over time.\n",
    "\n",
    "5. **Automation at Scale**:\n",
    "   - Processes millions of rows programmatically, ensuring scalability and adaptability to new datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Details to Remember**\n",
    "- **RapidFuzz for Matching**:\n",
    "  - Faster and more accurate than legacy approaches like `fuzzywuzzy`.\n",
    "  - Used to match terms dynamically during processing.\n",
    "\n",
    "- **Normalization Dictionary**:\n",
    "  - Acts as a single source of truth for term standardization.\n",
    "  - Periodic updates from `new_terms_log.txt` ensure continuous improvement.\n",
    "\n",
    "- **Output Organization**:\n",
    "  - All outputs are saved in a dedicated `/normalized` directory for easier management:\n",
    "    - `relationships_with_normalization.csv`: Knowledge graph relationships.\n",
    "    - `enriched_metadata_with_normalization.csv`: Metadata with normalized concepts.\n",
    "    - `new_terms_log.txt`: New terms for review.\n",
    "\n",
    "- **Future-Proofing**:\n",
    "  - Hierarchical graph relationships (e.g., `BELONGS_TO`, `HAS_CONCEPT`) support integration of additional agencies or divisions without restructuring.\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps for Ongoing Improvement**\n",
    "1. **Review New Terms Regularly**:\n",
    "   - Check `new_terms_log.txt` and update the normalization dictionary to improve automation accuracy.\n",
    "\n",
    "2. **Monitor for Edge Cases**:\n",
    "   - Analyze logs to identify terms that donâ€™t match well or require additional refinement.\n",
    "\n",
    "3. **Expand Normalization**:\n",
    "   - As datasets grow, refine and expand the dictionary to accommodate new agencies, divisions, or data types.\n",
    "\n",
    "4. **Ensure Query Efficiency**:\n",
    "   - With the hierarchical structure in place, ensure the graph database supports fast and intuitive querying.\n",
    "\n",
    "---\n",
    "\n",
    "This approach balances flexibility, scalability, and maintainability, ensuring the pipeline can handle diverse and expanding datasets without manual intervention. ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8f5d8-ee20-4ccb-ac1f-8c5a9e5d84f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "422f430b-3e32-4dc8-ad83-ea0f53758ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships for Knowledge Graph:\n",
      "        source                     target relationship source_type target_type\n",
      "0   Metadata_1             [Demographics]   BELONGS_TO    Metadata    Category\n",
      "1   Metadata_1           Annual Estimates  HAS_CONCEPT    Metadata     Concept\n",
      "2   Metadata_1        Resident Population  HAS_CONCEPT    Metadata     Concept\n",
      "3   Metadata_1       Five-Year Age Groups  HAS_CONCEPT    Metadata     Concept\n",
      "4   Metadata_1                        Sex  HAS_CONCEPT    Metadata     Concept\n",
      "5   Metadata_1  Municipios of Puerto Rico  HAS_CONCEPT    Metadata     Concept\n",
      "6   Metadata_1              Census Bureau  HAS_CONCEPT    Metadata     Concept\n",
      "7   Metadata_1        Population Division  HAS_CONCEPT    Metadata     Concept\n",
      "8   Metadata_2             [Demographics]   BELONGS_TO    Metadata    Category\n",
      "9   Metadata_2       Projected Population  HAS_CONCEPT    Metadata     Concept\n",
      "10  Metadata_2                 Age Groups  HAS_CONCEPT    Metadata     Concept\n",
      "11  Metadata_2                        Sex  HAS_CONCEPT    Metadata     Concept\n",
      "12  Metadata_2                       Race  HAS_CONCEPT    Metadata     Concept\n",
      "13  Metadata_2            Hispanic Origin  HAS_CONCEPT    Metadata     Concept\n",
      "14  Metadata_2              United States  HAS_CONCEPT    Metadata     Concept\n",
      "15  Metadata_2                  2014-2060  HAS_CONCEPT    Metadata     Concept\n",
      "16  Metadata_2              Census Bureau  HAS_CONCEPT    Metadata     Concept\n",
      "17  Metadata_2        Population Division  HAS_CONCEPT    Metadata     Concept\n",
      "18  Metadata_3             [Demographics]   BELONGS_TO    Metadata    Category\n",
      "19  Metadata_3           Projected Births  HAS_CONCEPT    Metadata     Concept\n",
      "20  Metadata_3                        Sex  HAS_CONCEPT    Metadata     Concept\n",
      "21  Metadata_3                       Race  HAS_CONCEPT    Metadata     Concept\n",
      "22  Metadata_3            Hispanic Origin  HAS_CONCEPT    Metadata     Concept\n",
      "23  Metadata_3              United States  HAS_CONCEPT    Metadata     Concept\n",
      "24  Metadata_3                  2014-2060  HAS_CONCEPT    Metadata     Concept\n",
      "25  Metadata_3              Census Bureau  HAS_CONCEPT    Metadata     Concept\n",
      "26  Metadata_3        Population Division  HAS_CONCEPT    Metadata     Concept\n",
      "27  Metadata_4             [Demographics]   BELONGS_TO    Metadata    Category\n",
      "28  Metadata_4           Projected Deaths  HAS_CONCEPT    Metadata     Concept\n",
      "29  Metadata_4         Single Year of Age  HAS_CONCEPT    Metadata     Concept\n",
      "30  Metadata_4                        Sex  HAS_CONCEPT    Metadata     Concept\n",
      "31  Metadata_4                       Race  HAS_CONCEPT    Metadata     Concept\n",
      "32  Metadata_4            Hispanic Origin  HAS_CONCEPT    Metadata     Concept\n",
      "33  Metadata_4              United States  HAS_CONCEPT    Metadata     Concept\n",
      "34  Metadata_4                  2014-2060  HAS_CONCEPT    Metadata     Concept\n",
      "35  Metadata_4              Census Bureau  HAS_CONCEPT    Metadata     Concept\n",
      "36  Metadata_4        Population Division  HAS_CONCEPT    Metadata     Concept\n",
      "37  Metadata_5             [Demographics]   BELONGS_TO    Metadata    Category\n",
      "38  Metadata_5       Projected Population  HAS_CONCEPT    Metadata     Concept\n",
      "39  Metadata_5         Single Year of Age  HAS_CONCEPT    Metadata     Concept\n",
      "40  Metadata_5                        Sex  HAS_CONCEPT    Metadata     Concept\n",
      "41  Metadata_5                       Race  HAS_CONCEPT    Metadata     Concept\n",
      "42  Metadata_5            Hispanic Origin  HAS_CONCEPT    Metadata     Concept\n",
      "43  Metadata_5                   Nativity  HAS_CONCEPT    Metadata     Concept\n",
      "44  Metadata_5              United States  HAS_CONCEPT    Metadata     Concept\n",
      "45  Metadata_5                  2014-2060  HAS_CONCEPT    Metadata     Concept\n",
      "46  Metadata_5              Census Bureau  HAS_CONCEPT    Metadata     Concept\n",
      "47  Metadata_5        Population Division  HAS_CONCEPT    Metadata     Concept\n",
      "\n",
      "New Terms Logged for Review:\n",
      "['Annual Estimates', 'Resident Population', 'Five-Year Age Groups', 'Sex', 'Municipios of Puerto Rico', 'Projected Population', 'Age Groups', 'Race', 'Hispanic Origin', 'United States', '2014-2060', 'Projected Births', 'Projected Deaths', 'Single Year of Age', 'Nativity']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from rapidfuzz import process  # For dynamic dictionary matching\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your metadata descriptions into a DataFrame\n",
    "metadata_df = pd.DataFrame({\n",
    "    \"metadata\": [\n",
    "        \"Annual Estimates of the Resident Population by Five-Year Age Groups and Sex for the Municipios of Puerto Rico. Source: U.S. Census Bureau, Population Division.\",\n",
    "        \"Projected Population by Age Groups, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division.\",\n",
    "        \"Projected Births by Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division.\",\n",
    "        \"Projected Deaths by Single Year of Age, Sex, Race, and Hispanic Origin for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division.\",\n",
    "        \"Projected Population by Single Year of Age, Sex, Race, Hispanic Origin, and Nativity for the United States: 2014-2060. Source: U.S. Census Bureau, Population Division.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Predefined normalization dictionary\n",
    "normalization_dict = {\n",
    "    \"US Census Bureau\": \"Census Bureau\",\n",
    "    \"Population Division\": \"Population Division\"\n",
    "}\n",
    "\n",
    "# Dynamic term logs for review\n",
    "new_terms_log = []\n",
    "\n",
    "# Normalize text based on the dictionary\n",
    "def normalize_text(term, norm_dict):\n",
    "    # Use RapidFuzz for dynamic matching\n",
    "    best_match = process.extractOne(term, norm_dict.keys(), score_cutoff=80)\n",
    "    return norm_dict[best_match[0]] if best_match else term\n",
    "\n",
    "# Enrichment extraction via OpenAI API\n",
    "def extract_metadata_enrichment(description):\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the following from the metadata description in a structured format:\n",
    "        1. High-Level Category: The overarching category this description belongs to (e.g., Demographics, Economics, Housing).\n",
    "        2. Concepts: List key terms or phrases that describe the metadata in brackets, separated by commas (e.g., [Population, Hispanic Origin, Age Groups])\n",
    "        \n",
    "        Format your response exactly like this:\n",
    "        High-Level Category: [category]\n",
    "        Concepts: [concept1, concept2, concept3]\n",
    "        \n",
    "        Metadata: \"{description}\"\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant that extracts metadata enrichment in a structured format.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing metadata: {description}\\nError: {e}\")\n",
    "        return None\n",
    "\n",
    "# Parse enrichment and normalize concepts\n",
    "def parse_and_normalize(enrichment):\n",
    "    if enrichment is None:\n",
    "        return pd.Series([None, None])\n",
    "    try:\n",
    "        # Extract Category and Concepts\n",
    "        category_match = re.search(r'High-Level Category:\\s*(.+?)(?=\\n|$)', enrichment)\n",
    "        concepts_match = re.search(r'Concepts:\\s*\\[(.*?)\\]', enrichment)\n",
    "\n",
    "        category = category_match.group(1).strip() if category_match else None\n",
    "        concepts = [concept.strip() for concept in concepts_match.group(1).split(\",\")] if concepts_match else []\n",
    "\n",
    "        # Normalize concepts using the dictionary\n",
    "        normalized_concepts = [normalize_text(concept, normalization_dict) for concept in concepts]\n",
    "\n",
    "        # Log any new terms not in the dictionary\n",
    "        for concept in normalized_concepts:\n",
    "            if concept not in normalization_dict.values() and concept not in new_terms_log:\n",
    "                new_terms_log.append(concept)\n",
    "\n",
    "        return pd.Series([category, normalized_concepts])\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing enrichment: {e}\")\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "# Apply API enrichment and normalization\n",
    "metadata_df[\"Enrichment\"] = metadata_df[\"metadata\"].apply(extract_metadata_enrichment)\n",
    "metadata_df[[\"Category\", \"Concepts\"]] = metadata_df[\"Enrichment\"].apply(parse_and_normalize)\n",
    "\n",
    "# Relationships for the knowledge graph\n",
    "relationships = []\n",
    "\n",
    "for idx, row in metadata_df.iterrows():\n",
    "    metadata_description = row[\"metadata\"]\n",
    "    category_node = row[\"Category\"]\n",
    "    concept_nodes = row[\"Concepts\"]\n",
    "    metadata_id = f\"Metadata_{idx + 1}\"\n",
    "    \n",
    "    # Add BELONGS_TO relationships\n",
    "    if category_node:\n",
    "        relationships.append({\n",
    "            \"source\": metadata_id,\n",
    "            \"target\": category_node,\n",
    "            \"relationship\": \"BELONGS_TO\",\n",
    "            \"source_type\": \"Metadata\",\n",
    "            \"target_type\": \"Category\"\n",
    "        })\n",
    "\n",
    "    # Add HAS_CONCEPT relationships\n",
    "    if concept_nodes:\n",
    "        for concept in concept_nodes:\n",
    "            relationships.append({\n",
    "                \"source\": metadata_id,\n",
    "                \"target\": concept,\n",
    "                \"relationship\": \"HAS_CONCEPT\",\n",
    "                \"source_type\": \"Metadata\",\n",
    "                \"target_type\": \"Concept\"\n",
    "            })\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \"./normalized\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save relationships to the output directory\n",
    "relationships_df = pd.DataFrame(relationships)\n",
    "relationships_df.to_csv(os.path.join(output_dir, \"relationships_with_normalization.csv\"), index=False)\n",
    "\n",
    "# Save enriched metadata to the output directory\n",
    "metadata_df.to_csv(os.path.join(output_dir, \"enriched_metadata_with_normalization.csv\"), index=False)\n",
    "\n",
    "# Save new terms log to the output directory\n",
    "if new_terms_log:\n",
    "    with open(os.path.join(output_dir, \"new_terms_log.txt\"), \"w\") as f:\n",
    "        for term in new_terms_log:\n",
    "            f.write(f\"{term}\\n\")\n",
    "\n",
    "print(\"Relationships for Knowledge Graph:\")\n",
    "print(relationships_df)\n",
    "\n",
    "print(\"\\nNew Terms Logged for Review:\")\n",
    "print(new_terms_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481fbe40-9b65-4072-941f-407ce0b36910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hf_env)",
   "language": "python",
   "name": "hf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
